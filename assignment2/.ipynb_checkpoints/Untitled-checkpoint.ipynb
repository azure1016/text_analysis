{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liujinjian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def read_data(path_dict):\n",
    "    df_dict = {}\n",
    "    for name, path in path_dict.items():\n",
    "        df_dict[name] = pd.read_csv(path, sep = 'delimiter',header = None)\n",
    "    return df_dict\n",
    "\n",
    "def concat_to_sentences(word2d_dict):\n",
    "    sentences_list_dict = {}\n",
    "    for name, lst in word2d_dict.items():\n",
    "        stc_list = []\n",
    "        for line in lst:\n",
    "            stc_list.append(\"\".join(line))\n",
    "            #something = \" \".join(line)\n",
    "            #print(line)\n",
    "        sentences_list_dict[name] = stc_list\n",
    "    return sentences_list_dict\n",
    "\n",
    "def lists_to_n_grams(list_dict, N, M):\n",
    "    ngram_dict = {}\n",
    "    #parameters to tune: max_df, min_df, max_features\n",
    "    vec = CountVectorizer(ngram_range = (N,M))\n",
    "    vec.fit(list_dict['train'])\n",
    "    #1. I use another vectorizer instance for list without stopwors. Is it necessary?\n",
    "    #2. Only learn vocabulary from train list, is it enough?\n",
    "    vec_no_stopword = CountVectorizer(ngram_range = (N,M))\n",
    "    vec_no_stopword.fit(list_dict['train_no_stopword'])\n",
    "    for name, lst in list_dict.items():\n",
    "        if name.endswith(\"stopword\"):\n",
    "            ngram_dict[name] = vec_no_stopword.transform(lst)\n",
    "        else:\n",
    "            ngram_dict[name] = vec.transform(lst)\n",
    "    return ngram_dict\n",
    "\n",
    "#use Tf-Idf to better model the input\n",
    "#parameters to tune: norm, sublinear_tf\n",
    "\n",
    "\n",
    "def tfidf_ize(bow_dict):\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf.fit(bow_dict['train'])\n",
    "    tfidf_no_stopword = TfidfTransformer()\n",
    "    tfidf_no_stopword.fit(bow_dict['train_no_stopword'])\n",
    "    tfidf_dict = {}\n",
    "    for name, bow in bow_dict.items():\n",
    "        if \"stopword\" in name:\n",
    "            tfidf_dict[name] = tfidf_no_stopword.transform(bow)\n",
    "        else:\n",
    "            tfidf_dict[name] = tfidf.transform(bow)\n",
    "    return tfidf_dict\n",
    "\n",
    "def train_clf(data, df_dict):\n",
    "    result = {}\n",
    "    clf = MultinomialNB()\n",
    "    clf_no_stopword = MultinomialNB()\n",
    "    clf.fit(data['train'], df_dict['train']['target'])\n",
    "    predicted = clf.predict(data['test'])\n",
    "    accuracy = np.mean(predicted == df_dict['test']['target'])\n",
    "    result['all'] = accuracy\n",
    "    \n",
    "    clf_no_stopword.fit(data['train_no_stopword'], df_dict['train_no_stopword']['target'])\n",
    "    predicted = clf_no_stopword.predict(data['test_no_stopword'])\n",
    "    accuracy = np.mean(predicted == df_dict['test_no_stopword']['target'])\n",
    "    result['no_stopword'] = accuracy\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dp = \"../assignment1/\"\n",
    "ddp = dp +\"pos_gen/\"\n",
    "ddn = dp +\"neg_gen/\"\n",
    "\n",
    "pos_test_path = ddp + \"test.csv\"\n",
    "pos_train_path = ddp + \"train.csv\"\n",
    "pos_val_path = ddp + \"val.csv\"\n",
    "pos_test_no_stopword = ddp + \"test_no_stopword.csv\"\n",
    "pos_train_no_stopword = ddp + \"train_no_stopword.csv\"\n",
    "pos_val_no_stopword = ddp + \"val_no_stopword.csv\"\n",
    "\n",
    "neg_test_path = ddn+ \"test.csv\"\n",
    "neg_train_path = ddn + \"train.csv\"\n",
    "neg_val_path = ddn + \"val.csv\"\n",
    "neg_test_no_stopword = ddn + \"test_no_stopword.csv\"\n",
    "neg_train_no_stopword = ddn + \"train_no_stopword.csv\"\n",
    "neg_val_no_stopword = ddn + \"val_no_stopword.csv\"\n",
    "\n",
    "path_dict = {'pos_test':pos_test_path,\n",
    "'pos_train':  pos_train_path,\n",
    "'pos_val': pos_val_path,\n",
    "'pos_test_no_stopword': pos_test_no_stopword,\n",
    "'pos_train_no_stopword': pos_train_no_stopword,\n",
    "'pos_val_no_stopword': pos_val_no_stopword,\n",
    "'neg_test': neg_test_path,\n",
    "'neg_train': neg_train_path,\n",
    "'neg_val': neg_val_path,\n",
    "'neg_test_no_stopword': neg_test_no_stopword,\n",
    "'neg_train_no_stopword': neg_train_no_stopword,\n",
    "'neg_val_no_stopword': neg_val_no_stopword\n",
    "            }\n",
    "\n",
    "df_dict = read_data(path_dict)\n",
    "\n",
    "# assign labels for sentences.\n",
    "for name, df in df_dict.items():\n",
    "    if name.startswith(\"pos\"):\n",
    "        df['target'] = pd.Series(1, index = df.index)\n",
    "#         print(df.shape)\n",
    "    else:\n",
    "        df['target'] = pd.Series(0, index = df.index)\n",
    "        \n",
    "# merge neg dataframe and pos dataframe\n",
    "merged_df_dict = {}\n",
    "merged_df_dict['train'] = pd.concat([df_dict['neg_train'],df_dict[\"pos_train\"]]).reset_index(drop=True)\n",
    "merged_df_dict['val'] = pd.concat([df_dict['neg_val'],df_dict[\"pos_val\"]]).reset_index(drop=True)\n",
    "merged_df_dict['test'] = pd.concat([df_dict['neg_test'],df_dict[\"pos_test\"]]).reset_index(drop=True)\n",
    "merged_df_dict['train_no_stopword'] = pd.concat([df_dict['neg_train_no_stopword'],df_dict[\"pos_train_no_stopword\"]]).reset_index(drop=True)\n",
    "merged_df_dict['test_no_stopword'] = pd.concat([df_dict['neg_test_no_stopword'],df_dict[\"pos_test_no_stopword\"]]).reset_index(drop=True)\n",
    "merged_df_dict['val_no_stopword'] = pd.concat([df_dict['neg_val_no_stopword'],df_dict[\"pos_val_no_stopword\"]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([0, 'target'], dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_dict['test'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expect numerical feature vectors with a fixed size rather than the raw text documents with variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "train_list = merged_df_dict['train'].iloc[:,0].values.tolist()\n",
    "train_no_stopword_list = merged_df_dict['train_no_stopword'].iloc[:,0].values.tolist()\n",
    "\n",
    "val_list = merged_df_dict['val'].iloc[:,0].values.tolist()\n",
    "val_no_stopword_list = merged_df_dict['val_no_stopword'].iloc[:,0].values.tolist()\n",
    "\n",
    "test_list = merged_df_dict['test'].iloc[:,0].values.tolist()\n",
    "test_no_stopword_list = merged_df_dict['test_no_stopword'].iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2d_dict = {\"train\": train_list,\n",
    "            \"test\": test_list,\n",
    "            \"val\": val_list,\n",
    "            \"train_no_stopword\": train_no_stopword_list,\n",
    "            \"test_no_stopword\": test_no_stopword_list,\n",
    "            \"val_no_stopword\": val_no_stopword_list}\n",
    "\n",
    "list_dict = concat_to_sentences(word2d_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640000\n"
     ]
    }
   ],
   "source": [
    "print(len(list_dict['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_unigram_dict = lists_to_n_grams(list_dict, 1, 1)\n",
    "bow_hybr_gram_dict = lists_to_n_grams(list_dict, 1, 2)\n",
    "bow_bigram_dict = lists_to_n_grams(list_dict, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'val', 'train_no_stopword', 'test_no_stopword', 'val_no_stopword'])\n"
     ]
    }
   ],
   "source": [
    "print(bow_unigram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_unigram_dict = tfidf_ize(bow_unigram_dict)\n",
    "tfidf_hybr_gram_dict = tfidf_ize(bow_hybr_gram_dict)\n",
    "tfidf_bigram_dict = tfidf_ize(bow_bigram_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'val', 'train_no_stopword', 'test_no_stopword', 'val_no_stopword'])\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_unigram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unigram_acc = train_clf(tfidf_unigram_dict, merged_df_dict)\n",
    "hybrid_gram_acc = train_clf(tfidf_hybr_gram_dict, merged_df_dict)\n",
    "bigram_acc = train_clf(tfidf_bigram_dict, merged_df_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('all', 0.8045625), ('no_stopword', 0.80595)])\n",
      "dict_items([('all', 0.8212875), ('no_stopword', 0.823775)])\n",
      "dict_items([('all', 0.7856625), ('no_stopword', 0.7881125)])\n"
     ]
    }
   ],
   "source": [
    "# results before tuning\n",
    "for acc in [unigram_acc, hybrid_gram_acc, bigram_acc]:\n",
    "    print(acc.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-62626f9a8561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m text_clf = Pipeline([\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     ('vect', CountVectorizer()),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseCrossValidator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwith_metaclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Iterable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'signature'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf_no_stopword = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(bow_unigram_dict['train'], merged_df_dict['train']['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
